#!/usr/bin/env python3
"""
Pipeline SIMPLE pour MangeTaMain - Preprocessing complet du dataset
Injection directe dans Streamlit
"""


import os
import sys
from datetime import datetime
import logging
from multiprocessing import Pool, cpu_count
import pandas as pd
import yaml

from data_prepro import RecipePreprocessor
from data_load import fetch_data, load_data
# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('preprocessing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Imports locaux


def process_chunk(chunk_data):
    """Traite un chunk de recettes"""
    chunk, chunk_id = chunk_data

    try:
        preprocessor = RecipePreprocessor()
        processed = preprocessor.preprocess_dataframe(chunk)

        logger.info(f" Chunk {chunk_id}: {len(processed)} recettes traitÃ©es")
        return processed

    except Exception as e:
        logger.error(f" Erreur chunk {chunk_id}: {e}")
        return pd.DataFrame()


def run_complete_preprocessing():
    """
    Pipeline complet pour tout le dataset
    RÃ©sultat directement utilisable par Streamlit
    """

    logger.info(" PREPROCESSING COMPLET - MANGETAMAIN")
    start_time = datetime.now()

    # === Ã‰TAPE 1: CHARGEMENT DES DONNÃ‰ES ===
    logger.info(" 1. Chargement des donnÃ©es Kaggle...")

    with open('config.yaml', 'r') as f:
        config = yaml.safe_load(f)

    # TÃ©lÃ©charger et charger
    dataset_id = config['datasets']['recipes']['dataset_id']
    dataset_path = fetch_data(dataset_id)

    files_to_load = [
        config['datasets']['recipes']['file_name'],
        config['datasets']['interactions']['file_name']
    ]

    dfs = load_data(dataset_path, files_to_load)
    recipes_df = dfs['RAW_recipes.csv'].copy()
    interactions_df = dfs['RAW_interactions.csv'].copy()

    logger.info(" Dataset complet chargÃ©:")
    logger.info(f"   - Recettes: {len(recipes_df):,}")
    logger.info(f"   - Interactions: {len(interactions_df):,}")

    # === Ã‰TAPE 2: NETTOYAGE PRÃ‰ALABLE ===
    logger.info("ğŸ§¹ 2. Nettoyage prÃ©alable des donnÃ©es...")

    # Supprimer les recettes sans ingrÃ©dients
    initial_count = len(recipes_df)
    recipes_df = recipes_df.dropna(subset=['ingredients'])
    recipes_df = recipes_df[recipes_df['ingredients'] != '[]']
    # Au moins quelques caractÃ¨res
    recipes_df = recipes_df[recipes_df['ingredients'].str.len() > 10]

    logger.info(f" Filtrage: {initial_count:,} â†’ {len(recipes_df):,} recettes")

    # === Ã‰TAPE 3: PREPROCESSING PARALLÃˆLE ===
    logger.info("âš¡ 3. Preprocessing parallÃ¨le du dataset complet...")

    # Configuration parallÃ¨le
    n_cores = min(cpu_count() - 1, 8)  # Utiliser tous les cores - 1
    chunk_size = max(2000, len(recipes_df) // (n_cores * 3))  # Chunks optimaux

    logger.info(f" Configuration: {n_cores} cores, chunks de {chunk_size}")

    # CrÃ©er les chunks
    chunks = []
    for i in range(0, len(recipes_df), chunk_size):
        end_idx = min(i + chunk_size, len(recipes_df))
        chunk = recipes_df.iloc[i:end_idx].copy()
        chunks.append((chunk, i // chunk_size + 1))

    logger.info(f" {len(chunks)} chunks crÃ©Ã©s pour {len(recipes_df):,} recettes")

    # Traitement parallÃ¨le
    logger.info(" DÃ©marrage du traitement parallÃ¨le...")

    with Pool(n_cores) as pool:
        processed_chunks = pool.map(process_chunk, chunks)

    # Filtrer les chunks vides
    processed_chunks = [chunk for chunk in processed_chunks if not chunk.empty]

    logger.info(f" {len(processed_chunks)} chunks traitÃ©s avec succÃ¨s")

    # === Ã‰TAPE 4: ASSEMBLAGE FINAL ===
    logger.info(" 4. Assemblage des donnÃ©es preprocessÃ©es...")

    # Combiner tous les chunks
    processed_recipes = pd.concat(
        processed_chunks,
        ignore_index=True,
        sort=False)

    # Merger avec les donnÃ©es originales nÃ©cessaires pour Streamlit
    merge_columns = [
        'id',
        'name',
        'minutes',
        'n_steps',
        'description',
        'n_ingredients']
    processed_recipes = processed_recipes.merge(
        recipes_df[merge_columns].rename(columns={'id': 'recipe_id'}),
        on='recipe_id',
        how='left'
    )

    # Colonnes pour compatibilitÃ© avec le systÃ¨me de recommandation
    processed_recipes['id'] = processed_recipes['recipe_id']

    # S'assurer que la colonne normalized_ingredients existe
    if 'normalized_ingredients_list' in processed_recipes.columns:
        processed_recipes['normalized_ingredients'] = processed_recipes['normalized_ingredients_list']

    logger.info(f" Dataset final: {len(processed_recipes):,} recettes preprocessÃ©es")
    logger.info(f" Colonnes: {list(processed_recipes.columns)}")

    # === Ã‰TAPE 5: SAUVEGARDE POUR STREAMLIT ===
    logger.info(" 5. Sauvegarde pour injection Streamlit...")

    # CrÃ©er le rÃ©pertoire de sortie
    output_dir = "/shared_data"  # Dossier partagÃ© avec Streamlit
    os.makedirs(output_dir, exist_ok=True)

    # Sauvegarde principale (Pickle pour rapiditÃ©)
    recipes_path = os.path.join(output_dir, "recipes_processed.pkl")
    processed_recipes.to_pickle(recipes_path)

    interactions_path = os.path.join(output_dir, "interactions.pkl")
    interactions_df.to_pickle(interactions_path)

    # Sauvegarde CSV pour debug
    processed_recipes.to_csv(
        os.path.join(
            output_dir,
            "recipes_processed.csv"),
        index=False)
    interactions_df.to_csv(
        os.path.join(
            output_dir,
            "interactions.csv"),
        index=False)

    logger.info(f" DonnÃ©es sauvegardÃ©es dans {output_dir}")

    # === Ã‰TAPE 6: MÃ‰TADONNÃ‰ES ET VALIDATION ===
    logger.info(" 6. GÃ©nÃ©ration des mÃ©tadonnÃ©es...")

    # Validation rapide
    has_ingredients = processed_recipes['normalized_ingredients'].apply(
        lambda x: isinstance(x, list) and len(x) > 0
    ).sum()

    duration = datetime.now() - start_time
    # 6. GÃ©nÃ©ration des mÃ©tadonnÃ©es
    metadata = {
        'processing_date': datetime.now().isoformat(),
        'total_recipes_input': int(
            len(recipes_df)),
        'total_recipes_processed': int(
            len(processed_recipes)),
        'recipes_with_ingredients': int(has_ingredients),
        'total_interactions': int(
            len(interactions_df)),
        'processing_time_minutes': float(
            round(
                duration.total_seconds() / 60,
                2)),
        'cores_used': int(n_cores),
        'chunks_processed': int(
            len(processed_chunks)),
        'success_rate': float(
            round(
                (len(processed_recipes) / len(recipes_df)) * 100,
                2)),
        'ready_for_streamlit': True}

    # Sauvegarder les mÃ©tadonnÃ©es (format JSON plus fiable)
    metadata_path = os.path.join(output_dir, "preprocessing_metadata.json")
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)

    # === RÃ‰SUMÃ‰ FINAL ===
    logger.info("ğŸ‰ PREPROCESSING COMPLET TERMINÃ‰ !")
    logger.info(f"â±ï¸ DurÃ©e totale: {duration}")
    logger.info("ğŸ“Š RÃ©sultats:")
    logger.info(f"   - Recettes preprocessÃ©es: {len(processed_recipes):,}")
    logger.info(f"   - Avec ingrÃ©dients normalisÃ©s: {has_ingredients:,}")
    logger.info(f"   - Interactions: {len(interactions_df):,}")
    logger.info(f"   - Taux de succÃ¨s: {metadata['success_rate']}%")
    logger.info(f"   - Vitesse: {len(processed_recipes) / duration.total_seconds():.0f} recettes/seconde")
    logger.info(f"ğŸ¯ DonnÃ©es prÃªtes pour Streamlit dans {output_dir}")

    return metadata


def verify_streamlit_data():
    """VÃ©rification que les donnÃ©es sont prÃªtes pour Streamlit"""

    logger.info("ğŸ” VÃ©rification des donnÃ©es pour Streamlit...")

    try:
        # Charger les donnÃ©es
        recipes_path = "/shared_data/recipes_processed.pkl"
        interactions_path = "/shared_data/interactions.pkl"

        if not os.path.exists(recipes_path) or not os.path.exists(
                interactions_path):
            logger.error("âŒ Fichiers de donnÃ©es manquants")
            return False

        recipes = pd.read_pickle(recipes_path)
        interactions = pd.read_pickle(interactions_path)

        # VÃ©rifications essentielles
        required_columns = [
            'id',
            'recipe_id',
            'name',
            'normalized_ingredients']
        missing = [
            col for col in required_columns if col not in recipes.columns]

        if missing:
            logger.error(f"âŒ Colonnes manquantes: {missing}")
            return False

        # VÃ©rifier les ingrÃ©dients
        has_ingredients = recipes['normalized_ingredients'].apply(
            lambda x: isinstance(x, list) and len(x) > 0
        ).sum()

        logger.info("âœ… Validation rÃ©ussie:")
        logger.info(f"   - Recettes: {len(recipes):,}")
        logger.info(f"   - Avec ingrÃ©dients: {has_ingredients:,}")
        logger.info(f"   - Interactions: {len(interactions):,}")

        return True

    except Exception as e:
        logger.error(f"âŒ Erreur validation: {e}")
        return False


if __name__ == "__main__":
    """ExÃ©cution du pipeline complet"""

    try:
        # Pipeline complet
        metadata = run_complete_preprocessing()

        # VÃ©rification automatique
        if verify_streamlit_data():
            logger.info("ğŸŠ SUCCÃˆS COMPLET - Streamlit prÃªt !")
            logger.info(
                "ğŸš€ Vous pouvez maintenant lancer: docker-compose up streamlit-app")
        else:
            logger.error("âŒ ProblÃ¨me de validation des donnÃ©es")
            sys.exit(1)

    except Exception as e:
        logger.error(f"âŒ Ã‰CHEC du pipeline: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
